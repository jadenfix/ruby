# GemHub Services - Docker Compose Configuration
# Lane D: AI Layer & Observability
version: '3.8'

services:
  # Lane D - LLM Gateway (FastAPI + OpenAI)
  llm_gateway:
    build:
      context: ./services/llm_gateway
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL=${MODEL:-gpt-4o-mini}
      - PYTHONPATH=/app
    volumes:
      - llm_logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - vector_store
    networks:
      - gemhub

  # Lane D - Vector Store (FAISS + SQLite)
  vector_store:
    build:
      context: ./services/vector_store
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      - PYTHONPATH=/app
      - VECTOR_STORE_PATH=/app/data
    volumes:
      - vector_data:/app/data
      - vector_models:/app/models
    healthcheck:
      test: ["CMD", "python", "-c", "import faiss; print('FAISS OK')"]
      interval: 60s
      timeout: 30s
      retries: 3
    networks:
      - gemhub

volumes:
  vector_data:
    driver: local
  vector_models:
    driver: local
  llm_logs:
    driver: local

networks:
  gemhub:
    driver: bridge 